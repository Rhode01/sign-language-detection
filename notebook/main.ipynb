{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_voc_annotations(annotation_dir, image_dir, class_map):\n",
    "    image_paths = []\n",
    "    bbox_data = []\n",
    "    class_labels =[]\n",
    "    for xml_file in sorted(os.listdir(os.path.join(annotation_dir))):\n",
    "        if not xml_file.endswith(\".xml\"):\n",
    "            continue\n",
    "        tree = ET.parse(os.path.join(annotation_dir, xml_file))\n",
    "        root = tree.getroot()\n",
    "        image_file_name = root.find(\"filename\").text\n",
    "        path = os.path.join(image_dir,image_file_name)\n",
    "\n",
    "        size = root.find(\"size\")\n",
    "        img_width = int(size.find(\"width\").text)\n",
    "        img_height = int(size.find(\"height\").text)\n",
    "\n",
    "        obj = root.find(\"object\")\n",
    "        if obj is not None:\n",
    "            class_name = obj.find(\"name\").text\n",
    "            if class_name not in class_map:\n",
    "                continue\n",
    "            class_id = class_map[class_name]\n",
    "\n",
    "            bndbox = obj.find(\"bndbox\")\n",
    "            xmin = float(bndbox.find(\"xmin\").text) / img_width\n",
    "            ymin = float(bndbox.find(\"ymin\").text) / img_height\n",
    "            xmax = float(bndbox.find(\"xmax\").text) / img_width\n",
    "            ymax = float(bndbox.find(\"ymax\").text) / img_height\n",
    "            image_paths.append(path)\n",
    "            bbox_data.append([xmin, ymin, xmax, ymax])\n",
    "            class_labels.append(class_id)\n",
    "    return image_paths, bbox_data, class_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directory = \"../data/images\"\n",
    "annotations_directory = \"../data/Annotations\"\n",
    "class_map ={\"thank you\":0,\"open palm\":1,\"first\":2,\"okay\":3,\"call\":4}\n",
    "image_paths, bounding_box,class_labels= parse_voc_annotations(annotations_directory,image_directory,class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = tf.constant(image_paths)\n",
    "bbox_data = tf.constant(bounding_box, dtype=tf.float32)\n",
    "class_labels = tf.constant(class_labels, dtype=tf.int32)\n",
    "\n",
    "def load_and_preprocess_image(path, bbox, label):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_image(image, channels=3)\n",
    "    image.set_shape([None, None, 3])\n",
    "    image = tf.image.resize(image, [128, 128])\n",
    "    image = image / 255.0\n",
    "    label_one_hot = tf.one_hot(label, depth=len(class_map))\n",
    "    return image, {\"gesture\": label_one_hot, \"bbox\": bbox}\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_paths, bbox_data, class_labels))\n",
    "dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=56).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "DATASET_SIZE = len(image_paths)\n",
    "train_size = int(0.8 * DATASET_SIZE)\n",
    "\n",
    "train_ds = dataset.take(train_size)\n",
    "val_ds = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TakeDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), {'gesture': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None), 'bbox': TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)})>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D,BatchNormalization,Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(input_shape=(128,128,3), include_top=False, weights=\"imagenet\")\n",
    "base_model.trainable =False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(128,128,3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "bbox = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x)\n",
    "bbox = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(1e-4))(bbox)\n",
    "bbox = Dense(4, name='bbox', activation='sigmoid')(bbox)  \n",
    "\n",
    "cls = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(1e-4))(x)\n",
    "cls = Dense(len(class_map), name='gesture', activation='softmax')(cls)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[bbox, cls])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss={\n",
    "      'bbox': tf.keras.losses.mae,         \n",
    "      'gesture': 'categorical_crossentropy'\n",
    "    },\n",
    "    metrics={'gesture': 'accuracy'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - bbox_loss: 0.3915 - gesture_accuracy: 0.1592 - gesture_loss: 3.1247 - loss: 3.7943\n",
      "Epoch 1: loss improved from inf to 3.70452, saving model to ../trained_model/best_sign_language_model.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Rhodrick\\Machine Learning\\sign language detection\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4s/step - bbox_loss: 0.3933 - gesture_accuracy: 0.1602 - gesture_loss: 3.0122 - loss: 3.7644 \n",
      "Epoch 2/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - bbox_loss: 0.3153 - gesture_accuracy: 0.5203 - gesture_loss: 1.2227 - loss: 1.6639 \n",
      "Epoch 2: loss improved from 3.70452 to 1.63041, saving model to ../trained_model/best_sign_language_model.keras\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - bbox_loss: 0.3152 - gesture_accuracy: 0.5270 - gesture_loss: 1.1815 - loss: 1.6527  \n",
      "Epoch 3/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - bbox_loss: 0.2450 - gesture_accuracy: 0.7981 - gesture_loss: 0.8183 - loss: 0.8667 \n",
      "Epoch 3: loss improved from 1.63041 to 0.95270, saving model to ../trained_model/best_sign_language_model.keras\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - bbox_loss: 0.2416 - gesture_accuracy: 0.7934 - gesture_loss: 0.9278 - loss: 0.8954  \n",
      "Epoch 4/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - bbox_loss: 0.2254 - gesture_accuracy: 0.9282 - gesture_loss: 0.3442 - loss: 0.4806\n",
      "Epoch 4: loss improved from 0.95270 to 0.52675, saving model to ../trained_model/best_sign_language_model.keras\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - bbox_loss: 0.2331 - gesture_accuracy: 0.9251 - gesture_loss: 0.3935 - loss: 0.4960   \n",
      "Epoch 5/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - bbox_loss: 0.2094 - gesture_accuracy: 0.9573 - gesture_loss: 0.0974 - loss: 0.3207 \n",
      "Epoch 5: loss improved from 0.52675 to 0.32876, saving model to ../trained_model/best_sign_language_model.keras\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - bbox_loss: 0.2110 - gesture_accuracy: 0.9535 - gesture_loss: 0.1057 - loss: 0.3234  \n",
      "Epoch 6/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - bbox_loss: 0.1778 - gesture_accuracy: 0.9709 - gesture_loss: 0.0795 - loss: 0.2929 \n",
      "Epoch 6: loss improved from 0.32876 to 0.29293, saving model to ../trained_model/best_sign_language_model.keras\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - bbox_loss: 0.1770 - gesture_accuracy: 0.9716 - gesture_loss: 0.0804 - loss: 0.2929  \n",
      "Epoch 7/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - bbox_loss: 0.1617 - gesture_accuracy: 1.0000 - gesture_loss: 0.0200 - loss: 0.2174 \n",
      "Epoch 7: loss improved from 0.29293 to 0.21739, saving model to ../trained_model/best_sign_language_model.keras\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - bbox_loss: 0.1612 - gesture_accuracy: 1.0000 - gesture_loss: 0.0205 - loss: 0.2174  \n",
      "Epoch 8/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - bbox_loss: 0.1525 - gesture_accuracy: 1.0000 - gesture_loss: 0.0110 - loss: 0.1919 \n",
      "Epoch 8: loss improved from 0.21739 to 0.19459, saving model to ../trained_model/best_sign_language_model.keras\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - bbox_loss: 0.1546 - gesture_accuracy: 1.0000 - gesture_loss: 0.0122 - loss: 0.1928  \n",
      "Epoch 9/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - bbox_loss: 0.1408 - gesture_accuracy: 1.0000 - gesture_loss: 0.0186 - loss: 0.1947 \n",
      "Epoch 9: loss did not improve from 0.19459\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 723ms/step - bbox_loss: 0.1384 - gesture_accuracy: 1.0000 - gesture_loss: 0.0212 - loss: 0.1947\n",
      "Epoch 10/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - bbox_loss: 0.1427 - gesture_accuracy: 1.0000 - gesture_loss: 0.0070 - loss: 0.1789 \n",
      "Epoch 10: loss improved from 0.19459 to 0.18136, saving model to ../trained_model/best_sign_language_model.keras\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - bbox_loss: 0.1438 - gesture_accuracy: 1.0000 - gesture_loss: 0.0089 - loss: 0.1797  \n",
      "Epoch 11/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - bbox_loss: 0.1351 - gesture_accuracy: 1.0000 - gesture_loss: 0.0027 - loss: 0.1691\n",
      "Epoch 11: loss improved from 0.18136 to 0.17074, saving model to ../trained_model/best_sign_language_model.keras\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - bbox_loss: 0.1373 - gesture_accuracy: 1.0000 - gesture_loss: 0.0025 - loss: 0.1696  \n",
      "Epoch 12/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - bbox_loss: 0.1342 - gesture_accuracy: 1.0000 - gesture_loss: 0.0127 - loss: 0.1723\n",
      "Epoch 12: loss did not improve from 0.17074\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 843ms/step - bbox_loss: 0.1356 - gesture_accuracy: 1.0000 - gesture_loss: 0.0160 - loss: 0.1736\n",
      "Epoch 13/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - bbox_loss: 0.1269 - gesture_accuracy: 0.9730 - gesture_loss: 0.1974 - loss: 0.2165\n",
      "Epoch 13: loss did not improve from 0.17074\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 830ms/step - bbox_loss: 0.1275 - gesture_accuracy: 0.9640 - gesture_loss: 0.2624 - loss: 0.2343\n",
      "Epoch 14/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - bbox_loss: 0.1210 - gesture_accuracy: 1.0000 - gesture_loss: 0.0012 - loss: 0.1649     \n",
      "Epoch 14: loss improved from 0.17074 to 0.16236, saving model to ../trained_model/best_sign_language_model.keras\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - bbox_loss: 0.1177 - gesture_accuracy: 1.0000 - gesture_loss: 0.0014 - loss: 0.1640  \n",
      "Epoch 15/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - bbox_loss: 0.1226 - gesture_accuracy: 1.0000 - gesture_loss: 9.3384e-04 - loss: 0.1636\n",
      "Epoch 15: loss improved from 0.16236 to 0.16205, saving model to ../trained_model/best_sign_language_model.keras\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - bbox_loss: 0.1207 - gesture_accuracy: 1.0000 - gesture_loss: 9.4346e-04 - loss: 0.1631  \n",
      "Epoch 16/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - bbox_loss: 0.1125 - gesture_accuracy: 1.0000 - gesture_loss: 3.1614e-04 - loss: 0.1537 \n",
      "Epoch 16: loss improved from 0.16205 to 0.15187, saving model to ../trained_model/best_sign_language_model.keras\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - bbox_loss: 0.1103 - gesture_accuracy: 1.0000 - gesture_loss: 3.1886e-04 - loss: 0.1531  \n",
      "Epoch 17/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - bbox_loss: 0.1219 - gesture_accuracy: 1.0000 - gesture_loss: 0.0364 - loss: 0.1663  \n",
      "Epoch 17: loss did not improve from 0.15187\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 767ms/step - bbox_loss: 0.1228 - gesture_accuracy: 1.0000 - gesture_loss: 0.0482 - loss: 0.1697\n",
      "Epoch 18/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - bbox_loss: 0.1300 - gesture_accuracy: 1.0000 - gesture_loss: 2.5591e-04 - loss: 0.1655 \n",
      "Epoch 18: loss did not improve from 0.15187\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 733ms/step - bbox_loss: 0.1304 - gesture_accuracy: 1.0000 - gesture_loss: 2.7345e-04 - loss: 0.1656\n",
      "Epoch 19/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - bbox_loss: 0.1335 - gesture_accuracy: 1.0000 - gesture_loss: 9.8038e-04 - loss: 0.1695 \n",
      "Epoch 19: loss did not improve from 0.15187\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 714ms/step - bbox_loss: 0.1336 - gesture_accuracy: 1.0000 - gesture_loss: 0.0013 - loss: 0.1696   \n",
      "Epoch 20/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - bbox_loss: 0.1195 - gesture_accuracy: 1.0000 - gesture_loss: 0.0053 - loss: 0.1645     \n",
      "Epoch 20: loss did not improve from 0.15187\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 751ms/step - bbox_loss: 0.1161 - gesture_accuracy: 1.0000 - gesture_loss: 0.0070 - loss: 0.1641\n",
      "Epoch 21/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - bbox_loss: 0.1359 - gesture_accuracy: 1.0000 - gesture_loss: 5.4389e-04 - loss: 0.1688 \n",
      "Epoch 21: loss did not improve from 0.15187\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 736ms/step - bbox_loss: 0.1374 - gesture_accuracy: 1.0000 - gesture_loss: 6.7775e-04 - loss: 0.1692\n",
      "Epoch 22/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - bbox_loss: 0.1241 - gesture_accuracy: 1.0000 - gesture_loss: 0.0052 - loss: 0.1585     \n",
      "Epoch 22: loss did not improve from 0.15187\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 683ms/step - bbox_loss: 0.1254 - gesture_accuracy: 1.0000 - gesture_loss: 0.0068 - loss: 0.1593\n",
      "Epoch 23/25\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - bbox_loss: 0.1238 - gesture_accuracy: 1.0000 - gesture_loss: 6.7189e-04 - loss: 0.1580 \n",
      "Epoch 23: loss did not improve from 0.15187\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 673ms/step - bbox_loss: 0.1248 - gesture_accuracy: 1.0000 - gesture_loss: 8.2155e-04 - loss: 0.1583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x249cebcb5c0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data= val_ds,\n",
    "    epochs= 25,\n",
    "    callbacks =[\n",
    "            tf.keras.callbacks.EarlyStopping(monitor=\"gesture_loss\", restore_best_weights=True, mode=\"min\", patience=5),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\"../trained_model/best_sign_language_model.keras\", save_best_only=True,monitor=\"loss\", mode=\"min\", verbose=1)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for layer in base_model.layers[-20:]:\n",
    "#     layer.trainable = True\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(1e-5),  \n",
    "#     loss=model.loss,\n",
    "#     metrics=[[],['accuracy']]\n",
    "# )\n",
    "\n",
    "# model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data= val_ds,\n",
    "#     epochs= 25,\n",
    "#     callbacks =[\n",
    "#             tf.keras.callbacks.EarlyStopping(monitor=\"gesture_accuracy\", restore_best_weights=True, mode=\"max\", patience=5),\n",
    "#             tf.keras.callbacks.ModelCheckpoint(\"../trained_model/best_sign_language_model.keras\", save_best_only=True,monitor=\"loss\", mode=\"min\", verbose=1)\n",
    "#     ],\n",
    "#     verbose=1\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
